\section{Experimental  Evaluation}\label{sec:nestexpeval}
Through the following experiments we want to prove that \textit{(i)} both the time required to serialize our data structure and \textit{(ii)} the query plan provided for the graph nesting query,  outperform the same tasks performed on top on other databases, either graph, relational, or document oriented. For a first analysis we compare the loading time (the time required to store and index the data structure) and for the second one we time the milliseconds to perform the query  over the serialized and indexed data structure. Moreover, we shall jointly compare the time required to query the data with the time spent on the loading phase, because in some cases better query performances may be lead to the creation of several indices.


The query plans are evaluated with respect to the data representation independently to the data attached to it: in this case there is no additional penalty over carrying the values associated to the indices, and only the algorithms used within the data structures are compared. We choose a graph where vertices are only represented by their vertex ids and their label, and edges are represented only by the source and target vertex, alongside with the edge's label. Consequently, within the relational model the whole information can be stored only within one single table regarding the edges. Similar approaches are used in Virtuoso for representing RDF triple stores over a relational engine. We performed our benchmark using  a bibliography network using the gMark generator \cite{BBCFLA17}, as the previous dataset cannot be used for this other query task. To each \texttt{authorOf} relation a Zipf's Law distribution with parameter $2.5$ is associated to the ingoing distribution, while a normal distribution between 0 and 80 is associated to the outgoing distribution. Consequently, each vertex represents either an \texttt{Author} or an authored \texttt{Paper}, and are represented by distinct vertex ids. The resulting graph is represented as a list of triplets: source id, edge label (author of) and target id. The generator was configured to generate $8$ experiments by incrementally creating a graph with vertices with a power of $10$, that is from $10$ to $10^8$.

We performed our tests over a Lenovo ThinkPad P51 with a 3.00 GHz (until 4.00 GHz) Intel Xeon processor and 64 GB of RAM at 2.400 MHz. The tests were performed over a ferromagnetic Hard Disk at 5400 RPM with an NTFS File System. We evaluate THoSP using the two pattern matching queries provided in the running example. Given that the secondary memory representation is a simple extension of the one used for nested graphs, we assume that our data serialization is always outperforming with repsect to graph libraries as discussed in Subsection \vref{sec:qbench} for graph joins. We used default configurations for both \textbf{Neo4J}, \textbf{PostgreSQL} and \textbf{ArangoDB}, while we changed the cache buffer configurations for Virtuoso (as suggested in the configuration file) for 64 GB of RAM; we also kept  default multithreaded query execution plan. For Neo4J and PostgreSQL we kept the same default policies as depicted for  GCEA in Subsection \vref{sec:qplan}. Therefore, we must only describe the conditions under which we performed the time execution experiments for \textbf{ArangoDB}. Its AQL queries were evaluated directly through the \texttt{arangosh} client and benchmarked using the \texttt{getExtra()} method. Given that little documentation is provided with respect to the internal plan's implementations, we referred to the \texttt{explain} procedure provided by the shell itself. All the aforementioned conditions do not degradate the query evaluations. Last, given that all databases (except from Neo4J) were coded in C/C++ and that Neo4J provided the worst overall performances, we implemented our serialization and THoSP only in C++.


\begin{table*}[!t]
\centering
\begin{adjustbox}{max width=\textwidth}
	%\begin{minipage}[b]{\textwidth}
	\centering
		\begin{tabular}{@{}c|rrrr|r@{}}
			\toprule
			\multicolumn{1}{c}{\textbf{Operands Size}} & \multicolumn{5}{|c}{\textbf{Operand Loading Time (C/C++)} (ms)}  \\
			Vertices ($|V|$)  & {PostgreSQL} & {Virtuoso} & {ArangoDB}  &  {Neo4J (Java)} & {\textbf{Nested Graphs} (C++)}  \\
			\midrule
			$10$   & 8 & 3.67 & 43 & 3,951  & 0.23\\
			$10^2$  & 18 & 6.86 & 267 &  4,124 & 0.65\\
			$10^3$  & 45 & 23.53 & 1,285 & 5,256 & 5.54\\
			$10^4$   & 225 & 371.40 & 11,478 &  11,251 & 39.14\\
			$10^5$   & 1,877 & 3,510.96 & 135,595 &  1,193,492 & 376.07 \\
			$10^6$  & 19,076 & 34,636.80 & 1,362,734 & $>$1H & 4,016.06\\
			$10^7$   & 184,421 & 364,129.00 & $>$1H & $>$1H & 47,452.10\\
			$10^8$  & 1,982,393 & $>$1H & $>$1H & $>$1H & 527,326.00\\
			\bottomrule
		\end{tabular}
	}
	%\end{minipage}
	\subcaption{\textit{Operand Loading and Indexing Time}. PostgreSQL and Neo4J have transactions, while Virtuoso and ArangoDB are transactionless. Nested Graphs are our proposed method which is transactionless.}
	\label{tab:storeevaluation}
\end{adjustbox}

\begin{adjustbox}{max width=\textwidth}
	%\begin{minipage}[b]{\textwidth}
	\centering
		\begin{tabular}{@{}cr|rrrr|r@{}}
			\toprule
			\multicolumn{2}{c}{\textbf{Operands Size}} & \multicolumn{5}{|c}{\textbf{\textsc{Two HOp Separated Pattern} Time (C/C++)} (ms)}  \\
			Vertices  & Matched Graphs  &  \multirow{ 2}{*}{PostgreSQL} & \multirow{ 2}{*}{Virtuoso} & \multirow{ 2}{*}{ArangoDB}  &  \multirow{ 2}{*}{Neo4J (Java)} & \multirow{ 2}{*}{THoSP (C++)}  \\
			($|V|$) & ($|m_V(\nested)|+|m_E(\nested)|$) & & & & & \\
			\midrule
			$10$ & $3$  & 2.10 & 11 & 15.00 & 681.40  & 0.11\\
			$10^2$ & $58$  & 9.68 & 63 & 3.89 &  1,943.98 & 0.14\\
			$10^3$ & $968$  & 17.96 & 63 & 12.34 & $>$1H & 0.46\\
			$10^4$ & $8,683$  & 69.27 & 364 & 46.74 &  $>$1H & 4.07\\
			$10^5$ & $88,885$  & 294.23 & 4,153 & 508.87 &  $>$1H & 43.81 \\
			$10^6$ & $902,020$  & 2,611.48 & 50,341 & 7,212.19 & $>$1H & 563.02\\
			$10^7$ & $8,991,417$  & 25,666.14 & 672,273 & 922,590.00 & $>$1H & 8,202.93\\
			$10^8$ & $89,146,891$  & 396,523.88 & $>$1H & $>$1H & $>$1H & 91,834.20\\
			\bottomrule
		\end{tabular}
	}
\subcaption{\textit{Graph Nesting Time}. PLease note that the Graph Join Running Time. Each data management system is grouped by its graph query language implementation. This table clearly shows that the definition of our query plan clearly outperforms the default query plan implemented over those different graph query languages and databases.}\label{tab:querytimeeval}
	%\end{minipage}
\end{adjustbox}

\end{table*}

At first, we must discuss the operator loading time (Table \vref{tab:storeevaluation}), directly loaded from the data generated by gMark. We shall  compare Virtuoso and PostgreSQL first, since they are both based on a traditional relational database engine using one single table to store a graph. In both cases the graph was only represented in triples: Virtuoso automatically stores an RDF graph, while in PostgreSQL the graph was stored by a table of quadruples, where the first is an edge primary key and the remaining elements are the gMark representations. Given that Virtuoso is transactionless, it performed better at serializing and index data for very small data sets (from $10$ to $10^3$) while, afterwards, the triple indexing time takes over on the overall performances. On the other hand, ArangoDB has not a relational data representation, and it just serializes the data as JSON objects to which several indices are associated. Given that the only data used to serialize data into ArangoDB are the edges' labels, all the time required to store the data is the indexing time. On the other hand, the Neo4J serialization proves to be inefficient, both because there are no constraints for data duplication and we must always check if the to-be-inserted vertex already exists, and because Lucene invertex indices are more useful to index full text documents than indexing graph data. Finally, our nested graph data structure creates adjacency lists directly when serializing the data, while primary indices are only serialized for granting other possible data accesses, even though they are not directly used by the THoSP algorithm. Please also note that no external primary index will be used during the THoSP query evaluation, given that only the adjacency lists information is required to join the edges in a two hop distance scenario.

Let us now consider the graph nesting time (Table \vref{tab:querytimeeval}): albeit no specific triplet or key are associated to the stored graph, PostgreSQL appears to be more performant than Virtuoso in performing graph queries. Please also note that the Virtuoso query engine rewrites the SPARQL query into SQL and, hereby, two SQL queries were performed in both cases. Since both data were represented in a similar way in secondary memory, the completely different performance between the two databases must be attributed  to an inefficient rewriting of the SPARQL query into SQL. In particular, the nested representation using JSON array for PostgreSQL proved to be more efficient than returning a full RDF graph represented as triplets, thus arguing in favour of document stores. The PostgreSQL's efficiency is attributable to the run-time indexing time of the relational tables, that is shared with ArangoDB, where the indices are created ad loading time instead: in both cases a single join operation is performed, plus some (either runtime or stored) index access time. Both PostgreSQL and ArangoDB use GroupBy-s to create collections of nested values, separately for both vertices and edges. As observed in the previous paragraph, no primary index is used while performing the THoSP query, and adjacency graphs are returned using the same data structure used for graph joins: one single vertex is returned alongside the set of outgoing edges. Moreover, the nesting result is not created by using group by-s, but by sparsely creating an index that associates the container to its content: as a result, our query plan does not generate an additional cost for sorting and collecting all the elements because it is provided during the graph traversal phase. Thus, the choice of representing the nesting information as a separate index proves to be particularly efficient.

